{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Driven Stock Analysis: Data Preprocessing Notebook\n",
    "## Nifty 50 Stock Market Analysis - Data Extraction & Transformation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Data Extraction Function from YAML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "def load_yaml_data(file_path):\n",
    "    \"\"\"Load YAML file safely\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return yaml.safe_load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_stock_data_from_yaml(data_folder_path):\n",
    "    \"\"\"\n",
    "    Extract stock data from YAML files organized by month and date\n",
    "    Expected structure:\n",
    "        data/\n",
    "          2023-10/\n",
    "            20231003.yaml   # contains list of dicts with keys: Ticker, open, high, ...\n",
    "    \"\"\"\n",
    "    all_stock_data = {}\n",
    "\n",
    "    data_path = Path(data_folder_path)\n",
    "    if not data_path.exists():\n",
    "        print(f\"Data folder not found at {data_folder_path}\")\n",
    "        return all_stock_data\n",
    "\n",
    "    for month_folder in sorted(data_path.iterdir()):\n",
    "        if month_folder.is_dir():\n",
    "            print(f\"Processing month: {month_folder.name}\")\n",
    "\n",
    "            for yaml_file in sorted(month_folder.glob('*.yaml')):\n",
    "                yaml_data = load_yaml_data(yaml_file)\n",
    "\n",
    "                if not yaml_data:\n",
    "                    continue\n",
    "\n",
    "                # yaml_data is expected to be a list of records\n",
    "                for record in yaml_data:\n",
    "                    # handle both 'Ticker' and 'Symbol' just in case\n",
    "                    symbol = record.get('Ticker') or record.get('Symbol')\n",
    "                    if symbol is None:\n",
    "                        continue\n",
    "\n",
    "                    if symbol not in all_stock_data:\n",
    "                        all_stock_data[symbol] = []\n",
    "\n",
    "                    # parse fields safely\n",
    "                    row = {\n",
    "                        \"Date\": record.get(\"date\") or yaml_file.stem,  # use date if present\n",
    "                        \"Symbol\": symbol,\n",
    "                        \"Open\": float(record.get(\"open\", 0)),\n",
    "                        \"High\": float(record.get(\"high\", 0)),\n",
    "                        \"Low\": float(record.get(\"low\", 0)),\n",
    "                        \"Close\": float(record.get(\"close\", 0)),\n",
    "                        \"Volume\": int(record.get(\"volume\", 0)),\n",
    "                        \"Month\": record.get(\"month\"),\n",
    "                    }\n",
    "                    all_stock_data[symbol].append(row)\n",
    "\n",
    "    return all_stock_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Data from YAML Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stock data from YAML files...\n",
      "Processing month: 2023-10\n",
      "Processing month: 2023-11\n",
      "Processing month: 2023-12\n",
      "Processing month: 2024-01\n",
      "Processing month: 2024-02\n",
      "Processing month: 2024-03\n",
      "Processing month: 2024-04\n",
      "Processing month: 2024-05\n",
      "Processing month: 2024-06\n",
      "Processing month: 2024-07\n",
      "Processing month: 2024-08\n",
      "Processing month: 2024-09\n",
      "Processing month: 2024-10\n",
      "Processing month: 2024-11\n",
      "\n",
      "✓ Extracted data for 50 stocks\n",
      "Sample symbols: ['SBIN', 'BAJFINANCE', 'TITAN', 'ITC', 'TCS']\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = r\"D:\\Guvi_projects\\Stock_market-analysis\\data\"\n",
    "\n",
    "print(\"Extracting stock data from YAML files...\")\n",
    "stock_data_raw = extract_stock_data_from_yaml(DATA_FOLDER)\n",
    "\n",
    "print(f\"\\n✓ Extracted data for {len(stock_data_raw)} stocks\")\n",
    "print(f\"Sample symbols: {list(stock_data_raw.keys())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert Raw Data to DataFrames and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols in raw data: 50\n",
      "✓ Created and cleaned 50 stock dataframes\n",
      "\n",
      "Sample data from SBIN:\n",
      "                 Date Symbol   Open    High     Low   Close    Volume  \\\n",
      "0 2023-10-03 05:30:00   SBIN  596.6  604.90  589.60  602.95  15322196   \n",
      "1 2023-10-04 05:30:00   SBIN  600.0  600.45  584.45  586.25  24914612   \n",
      "2 2023-10-05 05:30:00   SBIN  590.0  594.35  587.10  592.15  13248028   \n",
      "3 2023-10-06 05:30:00   SBIN  593.4  598.95  592.20  594.25   8216780   \n",
      "4 2023-10-09 05:30:00   SBIN  588.0  589.00  581.55  585.10   9189597   \n",
      "\n",
      "     Month  Daily_Return  Price_Change  \n",
      "0  2023-10           NaN          6.35  \n",
      "1  2023-10     -2.769716        -13.75  \n",
      "2  2023-10      1.006397          2.15  \n",
      "3  2023-10      0.354640          0.85  \n",
      "4  2023-10     -1.539756         -2.90  \n"
     ]
    }
   ],
   "source": [
    "# 4. Convert Raw Data to DataFrames and Clean\n",
    "\n",
    "def create_and_clean_dataframe(symbol_data):\n",
    "    \"\"\"\n",
    "    Create DataFrame from symbol data and perform cleaning.\n",
    "\n",
    "    Expects each item in symbol_data to have keys:\n",
    "    Date, Symbol, Open, High, Low, Close, Volume, (optional) Month\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(symbol_data)\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 1) Convert Date column (e.g. '2023-10-03 05:30:00') to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # 2) Drop rows with invalid dates\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 3) Sort by Date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # 4) Ensure numeric types for OHLCV\n",
    "    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 5) Drop rows with missing or non-positive OHLC\n",
    "    mask = (\n",
    "        (df['Open']  > 0) &\n",
    "        (df['High']  > 0) &\n",
    "        (df['Low']   > 0) &\n",
    "        (df['Close'] > 0)\n",
    "    )\n",
    "    df = df[mask].dropna(subset=['Open', 'High', 'Low', 'Close'])\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 6) Calculate daily returns (percentage) and price change\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    df['Daily_Return'] = df['Close'].pct_change() * 100.0\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Build cleaned DataFrames for each symbol from stock_data_raw\n",
    "stock_dataframes = {}\n",
    "\n",
    "print(f\"Symbols in raw data: {len(stock_data_raw)}\")\n",
    "for symbol, data in stock_data_raw.items():\n",
    "    df = create_and_clean_dataframe(data)\n",
    "    if not df.empty:\n",
    "        stock_dataframes[symbol] = df\n",
    "\n",
    "print(f\"✓ Created and cleaned {len(stock_dataframes)} stock dataframes\")\n",
    "\n",
    "# Safely show a sample if any exist\n",
    "if stock_dataframes:\n",
    "    first_symbol = list(stock_dataframes.keys())[0]\n",
    "    print(f\"\\nSample data from {first_symbol}:\")\n",
    "    print(stock_dataframes[first_symbol].head())\n",
    "else:\n",
    "    print(\"\\n⚠️ No valid stock dataframes were created. Check date parsing and OHLC filters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Master Consolidated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Master DataFrame created\n",
      "Shape: (14200, 10)\n",
      "\n",
      "Column Names: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Month', 'Daily_Return', 'Price_Change']\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14200 entries, 0 to 14199\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Date          14200 non-null  datetime64[ns]\n",
      " 1   Symbol        14200 non-null  object        \n",
      " 2   Open          14200 non-null  float64       \n",
      " 3   High          14200 non-null  float64       \n",
      " 4   Low           14200 non-null  float64       \n",
      " 5   Close         14200 non-null  float64       \n",
      " 6   Volume        14200 non-null  int64         \n",
      " 7   Month         14200 non-null  object        \n",
      " 8   Daily_Return  14150 non-null  float64       \n",
      " 9   Price_Change  14200 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6), int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "                 Date    Symbol     Open     High      Low    Close   Volume  \\\n",
      "0 2023-10-03 05:30:00  ADANIENT  2418.00  2424.90  2372.00  2387.25  2019899   \n",
      "1 2023-10-04 05:30:00  ADANIENT  2402.20  2502.75  2392.25  2464.95  2857377   \n",
      "2 2023-10-05 05:30:00  ADANIENT  2477.95  2486.50  2446.40  2466.35  1132455   \n",
      "3 2023-10-06 05:30:00  ADANIENT  2466.35  2514.95  2466.05  2478.10  1510035   \n",
      "4 2023-10-09 05:30:00  ADANIENT  2440.00  2459.70  2411.30  2442.60  1408224   \n",
      "\n",
      "     Month  Daily_Return  Price_Change  \n",
      "0  2023-10           NaN        -30.75  \n",
      "1  2023-10      3.254791         62.75  \n",
      "2  2023-10      0.056796        -11.60  \n",
      "3  2023-10      0.476413         11.75  \n",
      "4  2023-10     -1.432549          2.60  \n"
     ]
    }
   ],
   "source": [
    "# Combine all stock data into master dataframe\n",
    "master_df_list = []\n",
    "\n",
    "for symbol, df in stock_dataframes.items():\n",
    "    master_df_list.append(df)\n",
    "\n",
    "master_df = pd.concat(master_df_list, ignore_index=True)\n",
    "master_df = master_df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Master DataFrame created\")\n",
    "print(f\"Shape: {master_df.shape}\")\n",
    "print(f\"\\nColumn Names: {master_df.columns.tolist()}\")\n",
    "print(f\"\\nData Info:\")\n",
    "print(master_df.info())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(master_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load Sector Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sector data loaded\n",
      "\n",
      "Sector DataFrame shape: (50, 3)\n",
      "              COMPANY         sector                         Symbol\n",
      "0   ADANI ENTERPRISES  MISCELLANEOUS  ADANI ENTERPRISES: ADANIGREEN\n",
      "1   ADANI PORTS & SEZ  MISCELLANEOUS  ADANI PORTS & SEZ: ADANIPORTS\n",
      "2    APOLLO HOSPITALS  MISCELLANEOUS   APOLLO HOSPITALS: APOLLOHOSP\n",
      "3        ASIAN PAINTS         PAINTS       ASIAN PAINTS: ASIANPAINT\n",
      "4           AXIS BANK        BANKING            AXIS BANK: AXISBANK\n",
      "5          BAJAJ AUTO    AUTOMOBILES         BAJAJ AUTO: BAJAJ-AUTO\n",
      "6       BAJAJ FINANCE        FINANCE      BAJAJ FINANCE: BAJFINANCE\n",
      "7       BAJAJ FINSERV        FINANCE      BAJAJ FINSERV: BAJAJFINSV\n",
      "8  BHARAT ELECTRONICS        DEFENCE        BHARAT ELECTRONICS: BEL\n",
      "9       BHARTI AIRTEL        TELECOM          BHARTI AIRTEL: AIRTEL\n",
      "\n",
      "Unique sectors: 21\n",
      "\n",
      "Sector distribution:\n",
      "sector\n",
      "BANKING            6\n",
      "AUTOMOBILES        6\n",
      "SOFTWARE           5\n",
      "ENERGY             4\n",
      "MISCELLANEOUS      3\n",
      "PHARMACEUTICALS    3\n",
      "FINANCE            3\n",
      "RETAILING          2\n",
      "INSURANCE          2\n",
      "POWER              2\n",
      "STEEL              2\n",
      "FOOD & TOBACCO     2\n",
      "FMCG               2\n",
      "PAINTS             1\n",
      "TEXTILES           1\n",
      "TELECOM            1\n",
      "DEFENCE            1\n",
      "MINING             1\n",
      "ALUMINIUM          1\n",
      "ENGINEERING        1\n",
      "CEMENT             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load sector classification data\n",
    "sector_df = pd.read_csv('Sector_data - Sheet1.csv')\n",
    "\n",
    "print(\"✓ Sector data loaded\")\n",
    "print(f\"\\nSector DataFrame shape: {sector_df.shape}\")\n",
    "print(sector_df.head(10))\n",
    "print(f\"\\nUnique sectors: {sector_df['sector'].nunique()}\")\n",
    "print(f\"\\nSector distribution:\")\n",
    "print(sector_df['sector'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Merge Master Data with Sector Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1136 rows have missing sector information\n",
      "✓ Sector information merged successfully\n",
      "\n",
      "Sample merged data:\n",
      "                   Date      Symbol   Close   Volume         Sector\n",
      "284 2023-10-03 05:30:00  ADANIPORTS  831.40  2453090  MISCELLANEOUS\n",
      "285 2023-10-04 05:30:00  ADANIPORTS  824.60  4096647  MISCELLANEOUS\n",
      "286 2023-10-05 05:30:00  ADANIPORTS  825.20  2083505  MISCELLANEOUS\n",
      "287 2023-10-06 05:30:00  ADANIPORTS  830.75  1877058  MISCELLANEOUS\n",
      "288 2023-10-09 05:30:00  ADANIPORTS  790.05  5994282  MISCELLANEOUS\n",
      "289 2023-10-10 05:30:00  ADANIPORTS  819.50  7591327  MISCELLANEOUS\n",
      "290 2023-10-11 05:30:00  ADANIPORTS  814.85  3281387  MISCELLANEOUS\n",
      "291 2023-10-12 05:30:00  ADANIPORTS  814.95  2563606  MISCELLANEOUS\n",
      "292 2023-10-13 05:30:00  ADANIPORTS  813.75  3801000  MISCELLANEOUS\n",
      "293 2023-10-16 05:30:00  ADANIPORTS  805.65  2664794  MISCELLANEOUS\n"
     ]
    }
   ],
   "source": [
    "# Extract symbol mapping from sector data\n",
    "symbol_mapping = {}\n",
    "for idx, row in sector_df.iterrows():\n",
    "    symbol_part = row['Symbol'].split(':')[-1].strip()  # Extract symbol after colon\n",
    "    symbol_mapping[symbol_part] = row['sector']\n",
    "\n",
    "# Create sector mapping series\n",
    "sector_mapping_series = pd.Series(symbol_mapping)\n",
    "\n",
    "# Merge sector info with master dataframe\n",
    "master_df['Sector'] = master_df['Symbol'].map(symbol_mapping)\n",
    "\n",
    "# Handle missing sectors\n",
    "if master_df['Sector'].isna().sum() > 0:\n",
    "    print(f\"Warning: {master_df['Sector'].isna().sum()} rows have missing sector information\")\n",
    "    master_df = master_df.dropna(subset=['Sector'])\n",
    "\n",
    "print(\"✓ Sector information merged successfully\")\n",
    "print(f\"\\nSample merged data:\")\n",
    "print(master_df[['Date', 'Symbol', 'Close', 'Volume', 'Sector']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Calculate Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Yearly metrics calculated\n",
      "\n",
      "Top 10 Green Stocks (Gainers):\n",
      "        Symbol           Sector  Yearly_Return  Volatility\n",
      "43       TRENT        RETAILING     223.092613    2.307380\n",
      "7          BEL          DEFENCE     101.760057    2.328339\n",
      "27         M&M      AUTOMOBILES      95.976974    1.921277\n",
      "4   BAJAJ-AUTO      AUTOMOBILES      89.011153    1.760937\n",
      "32   POWERGRID            POWER      68.854924    1.867144\n",
      "8         BPCL           ENERGY      67.477150    2.206885\n",
      "17  HEROMOTOCO      AUTOMOBILES      58.976655    1.656324\n",
      "37   SUNPHARMA  PHARMACEUTICALS      57.282404    1.173282\n",
      "14     HCLTECH         SOFTWARE      53.257447    1.429465\n",
      "30        NTPC            POWER      51.513267    1.947501\n",
      "\n",
      "Top 10 Red Stocks (Losers):\n",
      "        Symbol          Sector  Yearly_Return  Volatility\n",
      "23         ITC  FOOD & TOBACCO       7.936327    1.194370\n",
      "28      MARUTI     AUTOMOBILES       6.926712    1.371249\n",
      "42       TITAN       RETAILING       3.518185    1.413589\n",
      "5   BAJAJFINSV         FINANCE       2.549566    1.411785\n",
      "25   KOTAKBANK         BANKING       1.991836    1.421566\n",
      "29   NESTLEIND  FOOD & TOBACCO       0.707141    1.251683\n",
      "19  HINDUNILVR            FMCG      -0.957916    1.207390\n",
      "6   BAJFINANCE         FINANCE     -16.110874    1.591849\n",
      "2   ASIANPAINT          PAINTS     -21.935046    1.266468\n",
      "21  INDUSINDBK         BANKING     -30.458409    1.911119\n"
     ]
    }
   ],
   "source": [
    "# Calculate yearly metrics for each stock\n",
    "yearly_metrics = []\n",
    "\n",
    "for symbol in master_df['Symbol'].unique():\n",
    "    symbol_data = master_df[master_df['Symbol'] == symbol].sort_values('Date')\n",
    "    \n",
    "    if len(symbol_data) > 0:\n",
    "        first_close = symbol_data['Close'].iloc[0]\n",
    "        last_close = symbol_data['Close'].iloc[-1]\n",
    "        \n",
    "        yearly_return = ((last_close - first_close) / first_close) * 100\n",
    "        volatility = symbol_data['Daily_Return'].std()\n",
    "        avg_volume = symbol_data['Volume'].mean()\n",
    "        avg_price = symbol_data['Close'].mean()\n",
    "        max_price = symbol_data['Close'].max()\n",
    "        min_price = symbol_data['Close'].min()\n",
    "        \n",
    "        sector = symbol_data['Sector'].iloc[0] if 'Sector' in symbol_data.columns else 'Unknown'\n",
    "        \n",
    "        yearly_metrics.append({\n",
    "            'Symbol': symbol,\n",
    "            'Sector': sector,\n",
    "            'Yearly_Return': yearly_return,\n",
    "            'Volatility': volatility,\n",
    "            'Avg_Price': avg_price,\n",
    "            'Max_Price': max_price,\n",
    "            'Min_Price': min_price,\n",
    "            'Avg_Volume': avg_volume,\n",
    "            'Start_Price': first_close,\n",
    "            'End_Price': last_close,\n",
    "            'Price_Change': last_close - first_close\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(yearly_metrics).sort_values('Yearly_Return', ascending=False)\n",
    "\n",
    "print(\"✓ Yearly metrics calculated\")\n",
    "print(f\"\\nTop 10 Green Stocks (Gainers):\")\n",
    "print(metrics_df[['Symbol', 'Sector', 'Yearly_Return', 'Volatility']].head(10))\n",
    "print(f\"\\nTop 10 Red Stocks (Losers):\")\n",
    "print(metrics_df[['Symbol', 'Sector', 'Yearly_Return', 'Volatility']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIAGNOSTIC: Stock Data Flow Analysis\n",
      "============================================================\n",
      "\n",
      "1. Stocks in raw data: 50\n",
      "2. Stocks in dataframes (after cleaning): 50\n",
      "3. Stocks in metrics: 46\n",
      "\n",
      "❌ Stocks FILTERED OUT during metrics calculation (4):\n",
      "   - ADANIENT\n",
      "     Reason: Not found in master_df\n",
      "   - BHARTIARTL\n",
      "     Reason: Not found in master_df\n",
      "   - BRITANNIA\n",
      "     Reason: Not found in master_df\n",
      "   - TATACONSUM\n",
      "     Reason: Not found in master_df\n",
      "\n",
      "============================================================\n",
      "ROOT CAUSE:\n",
      "============================================================\n",
      "The 4 missing stocks are NOT in the sector CSV file.\n",
      "This means the symbol mapping failed during the merge step.\n",
      "\n",
      "Let's check the sector CSV:\n",
      "Symbols in sector CSV: ['ADANIGREEN', 'ADANIPORTS', 'AIRTEL', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK', 'BAJAJ-AUTO', 'BAJAJFINSV', 'BAJFINANCE', 'BEL']... (50 total)\n",
      "\n",
      "Missing from sector mapping:\n",
      "  - ADANIENT\n",
      "  - BHARTIARTL\n",
      "  - BRITANNIA\n",
      "  - TATACONSUM\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check which stocks were filtered out\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC: Stock Data Flow Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stocks_in_raw = set(stock_data_raw.keys())\n",
    "stocks_in_dataframes = set(stock_dataframes.keys())\n",
    "stocks_with_metrics = set(metrics_df['Symbol'].unique())\n",
    "\n",
    "filtered_in_cleaning = stocks_in_raw - stocks_in_dataframes\n",
    "filtered_in_metrics = stocks_in_dataframes - stocks_with_metrics\n",
    "\n",
    "print(f\"\\n1. Stocks in raw data: {len(stocks_in_raw)}\")\n",
    "print(f\"2. Stocks in dataframes (after cleaning): {len(stocks_in_dataframes)}\")\n",
    "print(f\"3. Stocks in metrics: {len(stocks_with_metrics)}\")\n",
    "\n",
    "if filtered_in_cleaning:\n",
    "    print(f\"\\n❌ Stocks FILTERED OUT during data cleaning ({len(filtered_in_cleaning)}):\")\n",
    "    for symbol in sorted(filtered_in_cleaning):\n",
    "        print(f\"   - {symbol}\")\n",
    "        if symbol in stock_data_raw:\n",
    "            raw_count = len(stock_data_raw[symbol])\n",
    "            print(f\"     (had {raw_count} raw records)\")\n",
    "\n",
    "if filtered_in_metrics:\n",
    "    print(f\"\\n❌ Stocks FILTERED OUT during metrics calculation ({len(filtered_in_metrics)}):\")\n",
    "    for symbol in sorted(filtered_in_metrics):\n",
    "        print(f\"   - {symbol}\")\n",
    "        # Check why they were filtered\n",
    "        if symbol in master_df['Symbol'].values:\n",
    "            sector_value = master_df[master_df['Symbol'] == symbol]['Sector'].unique()[0]\n",
    "            print(f\"     Reason: Sector = {sector_value} (missing sector mapping)\")\n",
    "        else:\n",
    "            print(f\"     Reason: Not found in master_df\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROOT CAUSE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"The 4 missing stocks are NOT in the sector CSV file.\")\n",
    "print(\"This means the symbol mapping failed during the merge step.\")\n",
    "print(\"\\nLet's check the sector CSV:\")\n",
    "print(f\"Symbols in sector CSV: {sorted(symbol_mapping.keys())[:10]}... ({len(symbol_mapping)} total)\")\n",
    "print(f\"\\nMissing from sector mapping:\")\n",
    "for symbol in sorted(filtered_in_metrics):\n",
    "    print(f\"  - {symbol}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Generate Market Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Market Summary Generated\n",
      "\n",
      "==================================================\n",
      "MARKET OVERVIEW\n",
      "==================================================\n",
      "Total_Stocks: 46\n",
      "Green_Stocks: 42\n",
      "Red_Stocks: 4\n",
      "Green_Percentage: 91.30\n",
      "Red_Percentage: 8.70\n",
      "Avg_Return: 33.95\n",
      "Avg_Price: 2432.51\n",
      "Avg_Volume: 7197210.97\n",
      "Market_Return: 1561.77\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Market summary statistics\n",
    "green_stocks = (metrics_df['Yearly_Return'] > 0).sum()\n",
    "red_stocks = (metrics_df['Yearly_Return'] < 0).sum()\n",
    "total_stocks = len(metrics_df)\n",
    "\n",
    "avg_return = metrics_df['Yearly_Return'].mean()\n",
    "avg_price = master_df['Close'].mean()\n",
    "avg_volume = master_df['Volume'].mean()\n",
    "\n",
    "market_summary = {\n",
    "    'Total_Stocks': total_stocks,\n",
    "    'Green_Stocks': green_stocks,\n",
    "    'Red_Stocks': red_stocks,\n",
    "    'Green_Percentage': (green_stocks / total_stocks) * 100,\n",
    "    'Red_Percentage': (red_stocks / total_stocks) * 100,\n",
    "    'Avg_Return': avg_return,\n",
    "    'Avg_Price': avg_price,\n",
    "    'Avg_Volume': avg_volume,\n",
    "    'Market_Return': metrics_df['Yearly_Return'].sum()\n",
    "}\n",
    "\n",
    "print(\"✓ Market Summary Generated\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"MARKET OVERVIEW\")\n",
    "print(f\"{'='*50}\")\n",
    "for key, value in market_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Save Processed Data to CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data saved successfully to './processed_data' directory\n",
      "\n",
      "Files created:\n",
      "  - 46 individual stock CSV files\n",
      "  - consolidated_stock_data.csv (Master data)\n",
      "  - yearly_metrics.csv\n",
      "  - market_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = './processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save individual stock CSVs\n",
    "for symbol in master_df['Symbol'].unique():\n",
    "    symbol_data = master_df[master_df['Symbol'] == symbol]\n",
    "    filename = f\"{output_dir}/{symbol}_historical_data.csv\"\n",
    "    symbol_data.to_csv(filename, index=False)\n",
    "\n",
    "# Save consolidated data\n",
    "master_df.to_csv(f\"{output_dir}/consolidated_stock_data.csv\", index=False)\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv(f\"{output_dir}/yearly_metrics.csv\", index=False)\n",
    "\n",
    "# --------- SAFE MARKET SUMMARY (CAST TO PYTHON TYPES) ---------\n",
    "# Rebuild / cast market_summary to native Python types\n",
    "green_stocks = int((metrics_df['Yearly_Return'] > 0).sum())\n",
    "red_stocks   = int((metrics_df['Yearly_Return'] < 0).sum())\n",
    "total_stocks = int(len(metrics_df))\n",
    "\n",
    "avg_return = float(metrics_df['Yearly_Return'].mean())\n",
    "avg_price  = float(master_df['Close'].mean())\n",
    "avg_volume = float(master_df['Volume'].mean())\n",
    "market_return = float(metrics_df['Yearly_Return'].sum())\n",
    "\n",
    "market_summary = {\n",
    "    'Total_Stocks': total_stocks,\n",
    "    'Green_Stocks': green_stocks,\n",
    "    'Red_Stocks': red_stocks,\n",
    "    'Green_Percentage': float((green_stocks / total_stocks) * 100) if total_stocks else 0.0,\n",
    "    'Red_Percentage': float((red_stocks / total_stocks) * 100) if total_stocks else 0.0,\n",
    "    'Avg_Return': avg_return,\n",
    "    'Avg_Price': avg_price,\n",
    "    'Avg_Volume': avg_volume,\n",
    "    'Market_Return': market_return,\n",
    "}\n",
    "\n",
    "# Save market summary as JSON\n",
    "with open(f\"{output_dir}/market_summary.json\", 'w') as f:\n",
    "    json.dump(market_summary, f, indent=4)\n",
    "\n",
    "print(f\"✓ Data saved successfully to '{output_dir}' directory\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - {len(master_df['Symbol'].unique())} individual stock CSV files\")\n",
    "print(\"  - consolidated_stock_data.csv (Master data)\")\n",
    "print(\"  - yearly_metrics.csv\")\n",
    "print(\"  - market_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Calculate Additional Metrics for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cumulative returns calculated\n",
      "\n",
      "Cumulative returns for top 5 gainers:\n",
      "TRENT: 223.09%\n",
      "BEL: 101.76%\n",
      "M&M: 95.98%\n",
      "BAJAJ-AUTO: 89.01%\n",
      "POWERGRID: 68.85%\n"
     ]
    }
   ],
   "source": [
    "# Calculate cumulative returns for top performers\n",
    "top_5_gainers = metrics_df.head(5)['Symbol'].tolist()\n",
    "\n",
    "cumulative_returns_data = {}\n",
    "\n",
    "for symbol in master_df['Symbol'].unique():\n",
    "    symbol_data = master_df[master_df['Symbol'] == symbol].sort_values('Date')\n",
    "    \n",
    "    if len(symbol_data) > 0:\n",
    "        # Calculate cumulative return\n",
    "        symbol_data['Cumulative_Return'] = (1 + symbol_data['Daily_Return'] / 100).cumprod() - 1\n",
    "        cumulative_returns_data[symbol] = symbol_data[['Date', 'Symbol', 'Close', 'Cumulative_Return']]\n",
    "\n",
    "cumulative_df = pd.concat(cumulative_returns_data.values(), ignore_index=True)\n",
    "\n",
    "print(\"✓ Cumulative returns calculated\")\n",
    "print(f\"\\nCumulative returns for top 5 gainers:\")\n",
    "for symbol in top_5_gainers:\n",
    "    data = cumulative_df[cumulative_df['Symbol'] == symbol]\n",
    "    if not data.empty:\n",
    "        final_return = data['Cumulative_Return'].iloc[-1] * 100\n",
    "        print(f\"{symbol}: {final_return:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Sector-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sector-wise analysis completed\n",
      "\n",
      "Sector Performance:\n",
      "                 Avg_Return  Stock_Count  Avg_Volatility    Avg_Price  \\\n",
      "Sector                                                                  \n",
      "RETAILING            113.31            2            1.86   1012349.25   \n",
      "DEFENCE              101.76            1            2.33  28472446.58   \n",
      "POWER                 60.18            2            1.91  17084515.97   \n",
      "AUTOMOBILES           54.53            6            1.68   2936177.92   \n",
      "MINING                41.85            1            2.14  11918318.84   \n",
      "SOFTWARE              38.28            5            1.51   4404313.15   \n",
      "CEMENT                36.97            1            1.43    352559.78   \n",
      "ENERGY                36.56            3            1.93  15811123.87   \n",
      "MISCELLANEOUS         36.10            2            2.01   2623519.89   \n",
      "ALUMINIUM             35.87            1            1.96   6997897.32   \n",
      "TEXTILES              35.78            1            1.45    800653.35   \n",
      "PHARMACEUTICALS       31.38            3            1.35   2026204.96   \n",
      "STEEL                 19.22            2            1.73  23083669.53   \n",
      "ENGINEERING           17.25            1            1.70   2418356.50   \n",
      "INSURANCE             11.77            2            1.49   2864533.29   \n",
      "FINANCE               11.58            3            1.72   1389596.14   \n",
      "BANKING               11.38            6            1.55  11884644.18   \n",
      "FOOD & TOBACCO         4.32            2            1.22   7529864.41   \n",
      "FMCG                  -0.96            1            1.21   1872679.85   \n",
      "PAINTS               -21.94            1            1.27   1214614.35   \n",
      "\n",
      "                 Avg_Volume  \n",
      "Sector                       \n",
      "RETAILING           4080.88  \n",
      "DEFENCE              233.57  \n",
      "POWER                316.69  \n",
      "AUTOMOBILES         5395.56  \n",
      "MINING               435.59  \n",
      "SOFTWARE            1798.10  \n",
      "CEMENT             10265.76  \n",
      "ENERGY               644.57  \n",
      "MISCELLANEOUS       3725.97  \n",
      "ALUMINIUM            609.00  \n",
      "TEXTILES            2354.87  \n",
      "PHARMACEUTICALS     1396.79  \n",
      "STEEL                511.88  \n",
      "ENGINEERING         3483.05  \n",
      "INSURANCE           1085.62  \n",
      "FINANCE             3771.24  \n",
      "BANKING             1295.38  \n",
      "FOOD & TOBACCO      1472.36  \n",
      "FMCG                2533.72  \n",
      "PAINTS              3019.38  \n"
     ]
    }
   ],
   "source": [
    "# Sector-wise performance analysis\n",
    "sector_performance = metrics_df.groupby('Sector').agg({\n",
    "    'Yearly_Return': ['mean', 'count'],\n",
    "    'Volatility': 'mean',\n",
    "    'Avg_Volume': 'mean',\n",
    "    'Avg_Price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "sector_performance.columns = ['Avg_Return', 'Stock_Count', 'Avg_Volatility', 'Avg_Price', 'Avg_Volume']\n",
    "sector_performance = sector_performance.sort_values('Avg_Return', ascending=False)\n",
    "\n",
    "print(\"✓ Sector-wise analysis completed\")\n",
    "print(f\"\\nSector Performance:\")\n",
    "print(sector_performance)\n",
    "\n",
    "# Save sector performance\n",
    "sector_performance.to_csv(f\"{output_dir}/sector_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Correlation matrix calculated\n",
      "\n",
      "Correlation matrix shape: (46, 46)\n",
      "\n",
      "Sample correlations:\n",
      "Symbol      ADANIPORTS  APOLLOHOSP  ASIANPAINT  AXISBANK  BAJAJ-AUTO\n",
      "Symbol                                                              \n",
      "ADANIPORTS       1.000       0.815      -0.247     0.770       0.888\n",
      "APOLLOHOSP       0.815       1.000      -0.158     0.607       0.894\n",
      "ASIANPAINT      -0.247      -0.158       1.000     0.001      -0.163\n",
      "AXISBANK         0.770       0.607       0.001     1.000       0.743\n",
      "BAJAJ-AUTO       0.888       0.894      -0.163     0.743       1.000\n"
     ]
    }
   ],
   "source": [
    "# Create pivot table for correlation analysis\n",
    "pivot_close = master_df.pivot_table(\n",
    "    index='Date',\n",
    "    columns='Symbol',\n",
    "    values='Close'\n",
    ")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = pivot_close.corr().round(3)\n",
    "\n",
    "print(\"✓ Correlation matrix calculated\")\n",
    "print(f\"\\nCorrelation matrix shape: {correlation_matrix.shape}\")\n",
    "print(f\"\\nSample correlations:\")\n",
    "print(correlation_matrix.iloc[:5, :5])\n",
    "\n",
    "# Save correlation matrix\n",
    "correlation_matrix.to_csv(f\"{output_dir}/correlation_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Monthly Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Monthly performance analysis completed\n",
      "\n",
      "Monthly performance data points: 644\n",
      "\n",
      "Sample:\n",
      "  Month_Year      Symbol  Monthly_Return  Avg_Price     Volume\n",
      "0    2023-10  ADANIPORTS       -5.611018   801.1975   63920642\n",
      "1    2023-10  APOLLOHOSP       -5.842018  4978.7800    7880043\n",
      "2    2023-10  ASIANPAINT       -5.404424  3102.7800   18372958\n",
      "3    2023-10    AXISBANK       -5.686566   996.1975  170452203\n",
      "4    2023-10  BAJAJ-AUTO        5.932482  5178.2325   11508601\n",
      "5    2023-10  BAJAJFINSV        0.544505  1606.1500   32829911\n",
      "6    2023-10  BAJFINANCE       -5.961017  7857.2325   18942645\n",
      "7    2023-10         BEL       -4.274425   135.9725  218706151\n",
      "8    2023-10        BPCL        2.314272   172.3720  145737290\n",
      "9    2023-10       CIPLA        1.454177  1175.8950   34195095\n"
     ]
    }
   ],
   "source": [
    "# Add month-year column\n",
    "master_df['Month_Year'] = master_df['Date'].dt.to_period('M')\n",
    "\n",
    "# Monthly performance for each stock\n",
    "monthly_performance = []\n",
    "\n",
    "for month in sorted(master_df['Month_Year'].unique()):\n",
    "    month_data = master_df[master_df['Month_Year'] == month]\n",
    "    \n",
    "    for symbol in month_data['Symbol'].unique():\n",
    "        symbol_month_data = month_data[month_data['Symbol'] == symbol]\n",
    "        \n",
    "        if len(symbol_month_data) > 0:\n",
    "            first_price = symbol_month_data['Close'].iloc[0]\n",
    "            last_price = symbol_month_data['Close'].iloc[-1]\n",
    "            monthly_return = ((last_price - first_price) / first_price) * 100\n",
    "            \n",
    "            monthly_performance.append({\n",
    "                'Month_Year': str(month),\n",
    "                'Symbol': symbol,\n",
    "                'Monthly_Return': monthly_return,\n",
    "                'Avg_Price': symbol_month_data['Close'].mean(),\n",
    "                'Volume': symbol_month_data['Volume'].sum()\n",
    "            })\n",
    "\n",
    "monthly_df = pd.DataFrame(monthly_performance)\n",
    "\n",
    "print(\"✓ Monthly performance analysis completed\")\n",
    "print(f\"\\nMonthly performance data points: {len(monthly_df)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(monthly_df.head(10))\n",
    "\n",
    "# Save monthly performance\n",
    "monthly_df.to_csv(f\"{output_dir}/monthly_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Data Export for Streamlit Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All data processed and exported successfully!\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING SUMMARY\n",
      "==================================================\n",
      "Total stocks analyzed: 46\n",
      "Date range: 2023-10-03 05:30:00 to 2024-11-22 05:30:00\n",
      "Total data points: 13064\n",
      "Sectors covered: 20\n",
      "\n",
      "✓ All files saved in './processed_data' directory\n",
      "✓ Ready for Streamlit application!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Prepare data export for Streamlit app\n",
    "export_data = {\n",
    "    'master_data': master_df,\n",
    "    'metrics': metrics_df,\n",
    "    'correlation_matrix': correlation_matrix,\n",
    "    'monthly_performance': monthly_df,\n",
    "    'market_summary': market_summary\n",
    "}\n",
    "\n",
    "# Save as pickle for efficient loading in Streamlit\n",
    "import pickle\n",
    "\n",
    "with open(f\"{output_dir}/processed_data.pkl\", 'wb') as f:\n",
    "    pickle.dump(export_data, f)\n",
    "\n",
    "print(\"✓ All data processed and exported successfully!\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total stocks analyzed: {len(metrics_df)}\")\n",
    "print(f\"Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "print(f\"Total data points: {len(master_df)}\")\n",
    "print(f\"Sectors covered: {metrics_df['Sector'].nunique()}\")\n",
    "print(f\"\\n✓ All files saved in '{output_dir}' directory\")\n",
    "print(f\"✓ Ready for Streamlit application!\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SOLUTION: Adding Missing Stocks Sectors\n",
      "============================================================\n",
      "\n",
      "All symbols in sector CSV:\n",
      "                            Symbol           sector\n",
      "0    ADANI ENTERPRISES: ADANIGREEN    MISCELLANEOUS\n",
      "1    ADANI PORTS & SEZ: ADANIPORTS    MISCELLANEOUS\n",
      "2     APOLLO HOSPITALS: APOLLOHOSP    MISCELLANEOUS\n",
      "3         ASIAN PAINTS: ASIANPAINT           PAINTS\n",
      "4              AXIS BANK: AXISBANK          BANKING\n",
      "5           BAJAJ AUTO: BAJAJ-AUTO      AUTOMOBILES\n",
      "6        BAJAJ FINANCE: BAJFINANCE          FINANCE\n",
      "7        BAJAJ FINSERV: BAJAJFINSV          FINANCE\n",
      "8          BHARAT ELECTRONICS: BEL          DEFENCE\n",
      "9            BHARTI AIRTEL: AIRTEL          TELECOM\n",
      "10                      BPCL: BPCL           ENERGY\n",
      "11                    CIPLA: CIPLA  PHARMACEUTICALS\n",
      "12           COAL INDIA: COALINDIA           MINING\n",
      "13         DR. REDDYS LAB: DRREDDY  PHARMACEUTICALS\n",
      "14        EICHER MOTORS: EICHERMOT      AUTOMOBILES\n",
      "15                  GRASIM: GRASIM         TEXTILES\n",
      "16       HCL TECHNOLOGIES: HCLTECH         SOFTWARE\n",
      "17             HDFC BANK: HDFCBANK          BANKING\n",
      "18   HDFC LIFE INSURANCE: HDFCLIFE        INSURANCE\n",
      "19       HERO MOTOCORP: HEROMOTOCO      AUTOMOBILES\n",
      "20              HINDALCO: HINDALCO        ALUMINIUM\n",
      "21  HINDUSTAN UNILEVER: HINDUNILVR             FMCG\n",
      "22           ICICI BANK: ICICIBANK          BANKING\n",
      "23       INDUSIND BANK: INDUSINDBK          BANKING\n",
      "24                   INFOSYS: INFY         SOFTWARE\n",
      "25                        IOC: IOC           ENERGY\n",
      "26                        ITC: ITC   FOOD & TOBACCO\n",
      "27             JSW STEEL: JSWSTEEL            STEEL\n",
      "28  KOTAK MAHINDRA BANK: KOTAKBANK          BANKING\n",
      "29                         L&T: LT      ENGINEERING\n",
      "30                        M&M: M&M      AUTOMOBILES\n",
      "31           MARUTI SUZUKI: MARUTI      AUTOMOBILES\n",
      "32               NESTLE: NESTLEIND   FOOD & TOBACCO\n",
      "33                      NTPC: NTPC            POWER\n",
      "34                      ONGC: ONGC           ENERGY\n",
      "35           POWER GRID: POWERGRID            POWER\n",
      "36         RELIANCE IND.: RELIANCE           ENERGY\n",
      "37                       SBI: SBIN          BANKING\n",
      "38     SBI LIFE INSURANCE: SBILIFE        INSURANCE\n",
      "39     SHRIRAM FINANCE: SHRIRAMFIN          FINANCE\n",
      "40           SUN PHARMA: SUNPHARMA  PHARMACEUTICALS\n",
      "41     TATA CONSUMER: TATACONSUMER             FMCG\n",
      "42         TATA MOTORS: TATAMOTORS      AUTOMOBILES\n",
      "43           TATA STEEL: TATASTEEL            STEEL\n",
      "44                        TCS: TCS         SOFTWARE\n",
      "45            TECH MAHINDRA: TECHM         SOFTWARE\n",
      "46                    TITAN: TITAN        RETAILING\n",
      "47                    TRENT: TRENT        RETAILING\n",
      "48    ULTRATECH CEMENT: ULTRACEMCO           CEMENT\n",
      "49                    WIPRO: WIPRO         SOFTWARE\n",
      "\n",
      "\n",
      "Symbols in YAML data but missing from sector mapping:\n",
      "  - ADANIENT\n",
      "  - BHARTIARTL\n",
      "  - BRITANNIA\n",
      "  - TATACONSUM\n",
      "\n",
      "\n",
      "Manually adding sectors for missing stocks:\n",
      "  ADANIENT -> MISCELLANEOUS\n",
      "  BHARTIARTL -> TELECOM\n",
      "  BRITANNIA -> FOOD & TOBACCO\n",
      "  TATACONSUM -> FOOD & TOBACCO\n",
      "\n",
      "✓ All stocks now have sectors!\n",
      "\n",
      "Updated master_df size: 13064 rows\n",
      "Unique stocks now: 46\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Add missing stocks with their sectors\n",
    "print(\"=\"*60)\n",
    "print(\"SOLUTION: Adding Missing Stocks Sectors\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what's in sector_df\n",
    "print(\"\\nAll symbols in sector CSV:\")\n",
    "print(sector_df[['Symbol', 'sector']].to_string())\n",
    "\n",
    "print(\"\\n\\nSymbols in YAML data but missing from sector mapping:\")\n",
    "missing_stocks = []\n",
    "for symbol in sorted(filtered_in_metrics):\n",
    "    missing_stocks.append(symbol)\n",
    "    print(f\"  - {symbol}\")\n",
    "\n",
    "# Manual mapping for missing stocks (research these companies)\n",
    "missing_sector_mapping = {\n",
    "    'ADANIENT': 'MISCELLANEOUS',      # Adani Enterprises (parent company)\n",
    "    'BHARTIARTL': 'TELECOM',           # Bharti Airtel\n",
    "    'BRITANNIA': 'FOOD & TOBACCO',     # Britannia Industries\n",
    "    'TATACONSUM': 'FOOD & TOBACCO'     # Tata Consumer Products\n",
    "}\n",
    "\n",
    "print(f\"\\n\\nManually adding sectors for missing stocks:\")\n",
    "for symbol, sector in missing_sector_mapping.items():\n",
    "    print(f\"  {symbol} -> {sector}\")\n",
    "    \n",
    "# Update the symbol_mapping dictionary\n",
    "for symbol, sector in missing_sector_mapping.items():\n",
    "    symbol_mapping[symbol] = sector\n",
    "\n",
    "# Re-merge sector info with master_df\n",
    "master_df['Sector'] = master_df['Symbol'].map(symbol_mapping)\n",
    "\n",
    "# Check if all sectors are now mapped\n",
    "unmapped = master_df[master_df['Sector'].isna()]['Symbol'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"\\n⚠️ Still unmapped symbols: {unmapped}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All stocks now have sectors!\")\n",
    "\n",
    "# Remove any rows with missing sectors\n",
    "master_df = master_df.dropna(subset=['Sector'])\n",
    "\n",
    "print(f\"\\nUpdated master_df size: {len(master_df)} rows\")\n",
    "print(f\"Unique stocks now: {master_df['Symbol'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The following id_vars or value_vars are not present in the DataFrame: ['index']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m pd.DataFrame([market_summary]).to_csv(\u001b[33m'\u001b[39m\u001b[33mpowerbi_data/market_summary.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 5. Correlation Matrix (Unpivoted for Power BI)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m corr_unpivot = \u001b[43mcorrelation_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mindex\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStock2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCorrelation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m corr_unpivot.rename(columns={\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mStock1\u001b[39m\u001b[33m'\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m corr_unpivot.to_csv(\u001b[33m'\u001b[39m\u001b[33mpowerbi_data/correlations.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Guvi_projects\\Stock_market-analysis\\venv_stock\\Lib\\site-packages\\pandas\\core\\frame.py:9969\u001b[39m, in \u001b[36mDataFrame.melt\u001b[39m\u001b[34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[39m\n\u001b[32m   9959\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m] % {\u001b[33m\"\u001b[39m\u001b[33mcaller\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdf.melt(\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mother\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m   9960\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmelt\u001b[39m(\n\u001b[32m   9961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   9967\u001b[39m     ignore_index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   9968\u001b[39m ) -> DataFrame:\n\u001b[32m-> \u001b[39m\u001b[32m9969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9970\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcol_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Guvi_projects\\Stock_market-analysis\\venv_stock\\Lib\\site-packages\\pandas\\core\\reshape\\melt.py:74\u001b[39m, in \u001b[36mmelt\u001b[39m\u001b[34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing.any():\n\u001b[32m     71\u001b[39m     missing_labels = [\n\u001b[32m     72\u001b[39m         lab \u001b[38;5;28;01mfor\u001b[39;00m lab, not_found \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, missing) \u001b[38;5;28;01mif\u001b[39;00m not_found\n\u001b[32m     73\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m     75\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe following id_vars or value_vars are not present in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value_vars_was_not_none:\n\u001b[32m     79\u001b[39m     frame = frame.iloc[:, algos.unique(idx)]\n",
      "\u001b[31mKeyError\u001b[39m: \"The following id_vars or value_vars are not present in the DataFrame: ['index']\""
     ]
    }
   ],
   "source": [
    "# Add this at the end of your data processing script\n",
    "import os\n",
    "\n",
    "# Create export folder\n",
    "os.makedirs('powerbi_data', exist_ok=True)\n",
    "\n",
    "# 1. Master Data (Stock Prices)\n",
    "master_df.to_csv('powerbi_data/master_prices.csv', index=False)\n",
    "\n",
    "# 2. Metrics Summary (One row per stock)\n",
    "metrics_df.to_csv('powerbi_data/stock_metrics.csv', index=False)\n",
    "\n",
    "# 3. Monthly Performance\n",
    "monthly_df.to_csv('powerbi_data/monthly_performance.csv', index=False)\n",
    "\n",
    "# 4. Market Summary (Single row)\n",
    "pd.DataFrame([market_summary]).to_csv('powerbi_data/market_summary.csv', index=False)\n",
    "\n",
    "# 5. Correlation Matrix (Unpivoted for Power BI)\n",
    "corr_unpivot = correlation_matrix.reset_index().melt(\n",
    "    id_vars='index', \n",
    "    var_name='Stock2', \n",
    "    value_name='Correlation'\n",
    ")\n",
    "corr_unpivot.rename(columns={'index': 'Stock1'}, inplace=True)\n",
    "corr_unpivot.to_csv('powerbi_data/correlations.csv', index=False)\n",
    "\n",
    "print(\"✅ Data exported to 'powerbi_data' folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Export folder created: powerbi_data\n",
      "✅ Using existing data from memory\n",
      "\n",
      "🔍 Preparing data...\n",
      "\n",
      "📤 Exporting files...\n",
      "✅ 01_master_prices.csv (13,064 rows)\n",
      "✅ 02_stock_metrics.csv (46 rows)\n",
      "✅ 03_monthly_performance.csv (644 rows)\n",
      "✅ 04_market_summary.csv (1 row)\n",
      "\n",
      "❌ Export error: \"The following id_vars or value_vars are not present in the DataFrame: ['Stock1']\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9612\\2994706649.py\", line 118, in <module>\n",
      "    corr_unpivot = corr_reset.melt(\n",
      "        id_vars=['Stock1'],           # Column name is 'Stock1', not 'index'\n",
      "        var_name='Stock2',\n",
      "        value_name='Correlation'\n",
      "    )\n",
      "  File \"d:\\Guvi_projects\\Stock_market-analysis\\venv_stock\\Lib\\site-packages\\pandas\\core\\frame.py\", line 9969, in melt\n",
      "    return melt(\n",
      "           ~~~~^\n",
      "        self,\n",
      "        ^^^^^\n",
      "    ...<5 lines>...\n",
      "        ignore_index=ignore_index,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ).__finalize__(self, method=\"melt\")\n",
      "    ^\n",
      "  File \"d:\\Guvi_projects\\Stock_market-analysis\\venv_stock\\Lib\\site-packages\\pandas\\core\\reshape\\melt.py\", line 74, in melt\n",
      "    raise KeyError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "KeyError: \"The following id_vars or value_vars are not present in the DataFrame: ['Stock1']\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Data Export Script for Power BI - FIXED VERSION\n",
    "Handles all data structure variations\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Create export directory\n",
    "export_path = 'powerbi_data'\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "print(f\"📁 Export folder created: {export_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    _ = master_df\n",
    "    print(\"✅ Using existing data from memory\")\n",
    "except NameError:\n",
    "    print(\"🔄 Loading data from pickle file...\")\n",
    "    try:\n",
    "        with open('./processed_data/processed_data.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        master_df = data.get('master_data')\n",
    "        metrics_df = data.get('metrics')\n",
    "        correlation_matrix = data.get('correlation_matrix')\n",
    "        monthly_df = data.get('monthly_performance')\n",
    "        market_summary = data.get('market_summary')\n",
    "        \n",
    "        print(\"✅ Data loaded successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Error: processed_data.pkl not found!\")\n",
    "        exit()\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK AND PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🔍 Preparing data...\")\n",
    "\n",
    "# Create monthly_df if missing\n",
    "if 'monthly_df' not in locals() or monthly_df is None:\n",
    "    print(\"⚠️  Creating monthly_df from master_df...\")\n",
    "    try:\n",
    "        master_df['Date'] = pd.to_datetime(master_df['Date'])\n",
    "        master_df['Month_Year'] = master_df['Date'].dt.to_period('M').astype(str)\n",
    "        \n",
    "        monthly_df = master_df.groupby(['Month_Year', 'Symbol']).agg({\n",
    "            'Close': ['first', 'last']\n",
    "        }).reset_index()\n",
    "        \n",
    "        monthly_df.columns = ['Month_Year', 'Symbol', 'Open', 'Close']\n",
    "        monthly_df['Monthly_Return'] = ((monthly_df['Close'] - monthly_df['Open']) / monthly_df['Open']) * 100\n",
    "        print(\"✅ monthly_df created\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        monthly_df = pd.DataFrame(columns=['Month_Year', 'Symbol', 'Monthly_Return'])\n",
    "\n",
    "# Create market_summary if missing\n",
    "if 'market_summary' not in locals() or market_summary is None:\n",
    "    print(\"⚠️  Creating market_summary...\")\n",
    "    try:\n",
    "        total = len(metrics_df)\n",
    "        green = len(metrics_df[metrics_df['Yearly_Return'] > 0])\n",
    "        red = total - green\n",
    "        \n",
    "        market_summary = {\n",
    "            'Total_Stocks': total,\n",
    "            'Green_Stocks': green,\n",
    "            'Red_Stocks': red,\n",
    "            'Green_Percentage': (green/total)*100 if total > 0 else 0,\n",
    "            'Red_Percentage': (red/total)*100 if total > 0 else 0,\n",
    "            'Avg_Return': metrics_df['Yearly_Return'].mean()\n",
    "        }\n",
    "        print(\"✅ market_summary created\")\n",
    "    except:\n",
    "        market_summary = {}\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT TO CSV - CORRECTED VERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n📤 Exporting files...\")\n",
    "\n",
    "try:\n",
    "    # 1. Master Prices\n",
    "    master_df.to_csv(f'{export_path}/01_master_prices.csv', index=False)\n",
    "    print(f\"✅ 01_master_prices.csv ({len(master_df):,} rows)\")\n",
    "    \n",
    "    # 2. Stock Metrics\n",
    "    metrics_df.to_csv(f'{export_path}/02_stock_metrics.csv', index=False)\n",
    "    print(f\"✅ 02_stock_metrics.csv ({len(metrics_df):,} rows)\")\n",
    "    \n",
    "    # 3. Monthly Performance\n",
    "    monthly_df.to_csv(f'{export_path}/03_monthly_performance.csv', index=False)\n",
    "    print(f\"✅ 03_monthly_performance.csv ({len(monthly_df):,} rows)\")\n",
    "    \n",
    "    # 4. Market Summary\n",
    "    pd.DataFrame([market_summary]).to_csv(f'{export_path}/04_market_summary.csv', index=False)\n",
    "    print(f\"✅ 04_market_summary.csv (1 row)\")\n",
    "    \n",
    "    # 5. Correlation Matrix - FIXED VERSION\n",
    "    if correlation_matrix is not None:\n",
    "        # Ensure index has a name\n",
    "        if correlation_matrix.index.name is None:\n",
    "            correlation_matrix.index.name = 'Stock1'\n",
    "        \n",
    "        # Reset index to make Stock1 a column\n",
    "        corr_reset = correlation_matrix.reset_index()\n",
    "        \n",
    "        # Melt from wide to long format\n",
    "        corr_unpivot = corr_reset.melt(\n",
    "            id_vars=['Stock1'],           # Column name is 'Stock1', not 'index'\n",
    "            var_name='Stock2', \n",
    "            value_name='Correlation'\n",
    "        )\n",
    "        \n",
    "        # Remove self-correlations (optional, keeps file smaller)\n",
    "        corr_unpivot = corr_unpivot[corr_unpivot['Stock1'] != corr_unpivot['Stock2']]\n",
    "        \n",
    "        corr_unpivot.to_csv(f'{export_path}/05_correlations.csv', index=False)\n",
    "        print(f\"✅ 05_correlations.csv ({len(corr_unpivot):,} rows)\")\n",
    "    \n",
    "    # 6. Sector Summary\n",
    "    if 'Sector' in metrics_df.columns:\n",
    "        sector_summary = metrics_df.groupby('Sector').agg({\n",
    "            'Yearly_Return': ['mean', 'count', 'sum'],\n",
    "            'Volatility': 'mean',\n",
    "            'Symbol': lambda x: ','.join(x)\n",
    "        }).round(2)\n",
    "        sector_summary.columns = ['Avg_Return', 'Stock_Count', 'Total_Return', 'Avg_Volatility', 'Stocks']\n",
    "        sector_summary = sector_summary.reset_index()\n",
    "        sector_summary.to_csv(f'{export_path}/06_sector_summary.csv', index=False)\n",
    "        print(f\"✅ 06_sector_summary.csv ({len(sector_summary)} rows)\")\n",
    "    \n",
    "    # 7. Date Table\n",
    "    if 'Date' in master_df.columns:\n",
    "        dates_df = pd.DataFrame()\n",
    "        dates_df['Date'] = pd.to_datetime(master_df['Date']).unique()\n",
    "        dates_df = dates_df.sort_values('Date').drop_duplicates()\n",
    "        dates_df['Year'] = pd.to_datetime(dates_df['Date']).dt.year\n",
    "        dates_df['Month'] = pd.to_datetime(dates_df['Date']).dt.month\n",
    "        dates_df['Month_Name'] = pd.to_datetime(dates_df['Date']).dt.month_name()\n",
    "        dates_df['Quarter'] = pd.to_datetime(dates_df['Date']).dt.quarter\n",
    "        dates_df['Month_Year'] = pd.to_datetime(dates_df['Date']).dt.to_period('M').astype(str)\n",
    "        dates_df.to_csv(f'{export_path}/07_date_table.csv', index=False)\n",
    "        print(f\"✅ 07_date_table.csv ({len(dates_df):,} rows)\")\n",
    "\n",
    "    print(f\"\\n✨ EXPORT COMPLETE! Files saved to: '{export_path}/'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Export error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Correlations: 2070 rows\n",
      "Done! Check powerbi_data folder\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
